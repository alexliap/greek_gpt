{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67ffe376-ad36-4311-a3ce-918eaaaa8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with open(f\"data/text_data/train_text_dataset.pkl\", \"rb\") as file:\n",
    "    train_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0defbc84-07b8-4322-8a2c-d872aa53cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XGLMTokenizer, XGLMForCausalLM\n",
    "\n",
    "tokenizer = XGLMTokenizer.from_pretrained(\"facebook/xglm-564M\")\n",
    "model = XGLMForCausalLM.from_pretrained(\"facebook/xglm-564M\")\n",
    "\n",
    "# data_samples = {\n",
    "#     'el': [\n",
    "#         {\n",
    "#             \"premise\": \"Σαν επίθετο ελληνικά/ελληνικός σημαίνει από, ή σχετικά με την Ελλάδα, το λαό της, ή την κουλτούρα της.\",\n",
    "#             \"choice1\": \"Αρχαία ελληνική λογοτεχνία Ελληνική γλώσσα Ελληνική μυθολογία Ελληνική φιλοσοφία Ελληνικό αλφάβητο Ελληνικός καφές Ελληνικός πολιτισμός Ελληνική: Πρωινή εφημερίδα Αθηνών από το 1925.\",\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# prompt = data_samples['el'][0]['premise']\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# input_ids, output_ids = inputs[\"input_ids\"], inputs[\"input_ids\"][:, 1:]\n",
    "# outputs = model(**inputs, labels=input_ids)\n",
    "# logits = outputs.logits\n",
    "# logprobs = torch.gather(F.log_softmax(logits, dim=2), 2, output_ids.unsqueeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b068d9c3-46f4-4d42-9f22-6550bea9ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "toknized_text = tokenizer(train_data[0], return_tensors=\"pt\")\n",
    "input_ids, output_ids = toknized_text[\"input_ids\"], toknized_text[\"input_ids\"][:, 1:]\n",
    "outputs = model(**toknized_text, labels=input_ids)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "703df3b0-e86b-4747-ae1e-b877a1cc5457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-29.7202, -29.8207,  -3.3591,  ..., -29.4810, -29.9216, -29.5775],\n",
       "         [-37.7262, -38.5448,  -9.7600,  ..., -37.8513, -38.6274, -38.3732],\n",
       "         [-40.8652, -41.5271,  -9.3609,  ..., -40.8041, -41.8329, -41.0001],\n",
       "         ...,\n",
       "         [-39.2513, -39.8346,  -6.9689,  ..., -38.9556, -40.3755, -39.1697],\n",
       "         [-52.7946, -53.6060,  -7.2909,  ..., -52.4221, -54.0735, -52.9135],\n",
       "         [-33.3091, -34.0045,  -5.1757,  ..., -33.3538, -34.3533, -33.3771]]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(logits, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0896c106-2bd1-40aa-8627-1936f4290686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.7024, 23.7592, 17.5587,  ..., 18.5534, 18.1724, 13.6467],\n",
       "         [12.0696, 13.5225, 18.1403,  ..., 21.0680, 13.8093, 16.1025],\n",
       "         [16.4932,  9.5688, 18.4145,  ..., 20.8660, 10.4263, 20.6164],\n",
       "         ...,\n",
       "         [-0.4551,  3.1389,  9.3599,  ...,  7.9678,  4.7735,  7.7314],\n",
       "         [-1.0877, 10.4731,  6.8372,  ...,  9.9263, 15.2117, 11.8275],\n",
       "         [14.5214, 11.4354,  3.3154,  ..., -0.2465, -1.2460, -0.2698]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.view(1, logits.size(2), logits.size(1))[:, :, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "42db7ee8-2118-4501-835a-e3f255f6ddaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.36952209472656"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(\n",
    "    logits.view(1, logits.size(2), logits.size(1))[:, :, 1:], output_ids\n",
    ").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a8ede9d3-bef7-4327-a2fc-81213a70231b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean Cross Entropy: 0.498 | Mean PPL: 0.784:   0%|               | 1/181467 [00:19<967:27:39, 19.19s/it]                                             \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [1, 2047], got [1, 2048]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m         logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     29\u001b[0m         total_preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m output_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m         cross_entropy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m         log_prob_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(F\u001b[38;5;241m.\u001b[39msoftmax(logits, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m2\u001b[39m, output_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     34\u001b[0m ce_string \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Cross Entropy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m((cross_entropy\u001b[38;5;241m/\u001b[39mtotal_preds)\u001b[38;5;241m.\u001b[39mitem(),\u001b[38;5;250m \u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Projects/greek_gpt/.venv/lib/python3.12/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [1, 2047], got [1, 2048]"
     ]
    }
   ],
   "source": [
    "cross_entropy = 0\n",
    "total_preds = 0\n",
    "log_prob_sum = 0\n",
    "bar = tqdm(train_data, bar_format=\"{l_bar}{bar:15}{r_bar}{bar:-15b}\")\n",
    "for text in bar:\n",
    "    toknized_text = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    if toknized_text[\"input_ids\"].shape[1] < model.config.max_position_embeddings:\n",
    "        input_ids, output_ids = (\n",
    "            toknized_text[\"input_ids\"],\n",
    "            toknized_text[\"input_ids\"][:, 1:],\n",
    "        )\n",
    "\n",
    "        outputs = model(**toknized_text, labels=input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_preds += output_ids.shape[1]\n",
    "        cross_entropy += F.cross_entropy(\n",
    "            logits.view(1, logits.size(2), logits.size(1))[:, :, 1:], output_ids\n",
    "        )\n",
    "\n",
    "        log_prob_sum += torch.gather(\n",
    "            F.softmax(logits, 2), 2, output_ids.unsqueeze(2)\n",
    "        ).sum()\n",
    "    else:\n",
    "        diff = (\n",
    "            toknized_text[\"input_ids\"].shape[1] - model.config.max_position_embeddings\n",
    "        )\n",
    "        for i in range(toknized_text[\"input_ids\"].shape[1] - diff):\n",
    "            input_ids = toknized_text[\"input_ids\"][\n",
    "                :, i : i + model.config.max_position_embeddings\n",
    "            ]\n",
    "            output_ids = toknized_text[\"input_ids\"][\n",
    "                :, i + 1 : i + model.config.max_position_embeddings + 1\n",
    "            ]\n",
    "\n",
    "            attention_mask = toknized_text[\"attention_mask\"][\n",
    "                :, i : i + model.config.max_position_embeddings\n",
    "            ]\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, labels=input_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_preds += output_ids.shape[1]\n",
    "            cross_entropy += F.cross_entropy(\n",
    "                logits.view(1, logits.size(2), logits.size(1))[:, :, 1:], output_ids\n",
    "            )\n",
    "\n",
    "            log_prob_sum += torch.gather(\n",
    "                F.softmax(logits, 2), 2, output_ids.unsqueeze(2)\n",
    "            ).sum()\n",
    "\n",
    "    ce_string = f\"Mean Cross Entropy: {round((cross_entropy/total_preds).item(), 3)}\"\n",
    "    ppl_string = f\"Mean PPL: {round(torch.exp(-log_prob_sum/total_preds).item(), 3)}\"\n",
    "\n",
    "    bar.set_description(ce_string + \" | \" + ppl_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0be65463-0d8b-49fd-8f64-71e0a9f0128a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2281])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toknized_text[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "63e6f4d2-7d13-4bf0-8846-c9131e075435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acfcc9a-3f0a-45f8-b177-c2e5021656d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
