hydra:
  run:
    dir: outputs/pretrain/${hydra:runtime.choices.model}/
defaults:
  - _self_
  - model: greek_gpt_base

ckpt_path: null

train_dataloader:
  _target_: src.training.dataloader.CustomDataLoader
  data_path: "data/tokenized_data/train_tokenizer_5000_data.pkl"
  batch_size: 10
  block_size: 256
  is_test: true

val_dataloader:
  _target_: src.training.dataloader.CustomDataLoader
  data_path: "data/tokenized_data/val_tokenizer_5000_data.pkl"
  batch_size: 10
  block_size: 256
  is_test: true

trainer:
  _target_: lightning.Trainer
  accelerator: gpu
  strategy: auto
  devices: 1
  num_nodes: 1
  precision: 32
  num_sanity_val_steps: 0
  val_check_interval: 200
  enable_model_summary: true
  logger:
      _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: ${hydra:runtime.output_dir}
      name: logs
  callbacks:
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: ${hydra:runtime.output_dir}/checkpoints
      monitor: val_ce_loss
      save_weights_only: true
      filename: "greek_gpt-{step}-{val_ce_loss:.2f}"
      mode: min
      save_top_k: 5
      every_n_train_steps: 200
  #   - _target_: lightning.pytorch.callbacks.EarlyStopping
  #     monitor: val/PackedMSELoss
  #     min_delta: 1000
  #     patience: 20
  #     mode: min
  #     strict: false
  #     verbose: true
  max_epochs: 1
  enable_progress_bar: true
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
