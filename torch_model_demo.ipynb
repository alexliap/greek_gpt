{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fcdcb92-3e24-455c-a5a2-7a55a618f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "from torch_implementation.language_model import LanguageModel\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, BatchSampler\n",
    "import lightning as L\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c35bf4-d121-4fec-959e-9d01b9039ef3",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# TF_ENABLE_ONEDNN_OPTS=0\n",
    "# torch.set_float32_matmul_precision('medium')"
=======
    "TF_ENABLE_ONEDNN_OPTS = 0\n",
    "torch.set_float32_matmul_precision(\"medium\")"
>>>>>>> dd25b937ffe06cfe8aecfaacad3380e27dfa4f7e
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "522a2468-9fc2-4120-956a-1f59792caee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = \"tokenizer_5000\"\n",
    "\n",
    "with open(f\"data/tokenized_data/train_{tokenizer}_data.pkl\", \"rb\") as file:\n",
    "    train_data = pickle.load(file)\n",
    "# with open(f\"data/tokenized_data/test_{tokenizer}_data.pkl\", \"rb\") as file:\n",
    "#     test_data = pickle.load(file)\n",
    "\n",
    "train_data = [token for sents in train_data for token in sents]\n",
    "# test_data = [token for sents in test_data for token in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b6620ca-845f-49db-8998-9d0d6ecda2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_data)\n",
    "# test_data = torch.tensor(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b814973-9831-414c-a77f-8b267b177c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dt = TensorDataset(train_data[:-1], train_data[1:])\n",
    "seq_sampler = SequentialSampler(train_dt)\n",
<<<<<<< HEAD
    "batch_sampler = BatchSampler(seq_sampler, batch_size=64*256, drop_last=True)\n",
    "train_loader = DataLoader(train_dt, num_workers=os.cpu_count(), batch_sampler=batch_sampler, persistent_workers=True)\n",
=======
    "# rand_sampler = RandomSampler(train_dt, num_samples=3)\n",
    "batch_sampler = BatchSampler(seq_sampler, batch_size=128 * 256, drop_last=True)\n",
    "train_loader = DataLoader(\n",
    "    train_dt, num_workers=10, batch_sampler=batch_sampler, persistent_workers=True\n",
    ")  # , batch_size=128*256)\n",
>>>>>>> dd25b937ffe06cfe8aecfaacad3380e27dfa4f7e
    "\n",
    "model = LanguageModel(\n",
    "    vocab_size=5000,\n",
    "    n_embed=256,\n",
    "    context_len=256,\n",
    "    n_blocks=6,\n",
    "    n_heads=4,\n",
    "    n_experts=4,\n",
    "    top_k=2,\n",
    "    lr=3e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f27477-8118-4a3b-bd49-c74c6662d830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: C:\\Users\\Alex\\Desktop\\Projects\\greek_gpt\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type              | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | embed_layer      | Embedding         | 1.3 M  | train\n",
      "1 | positional_embed | Embedding         | 65.5 K | train\n",
      "2 | blocks           | TransformerBlocks | 7.9 M  | train\n",
      "3 | layer_norm       | LayerNorm         | 512    | train\n",
      "4 | llm_head         | Linear            | 1.3 M  | train\n",
      "---------------------------------------------------------------\n",
      "10.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.5 M    Total params\n",
      "42.137    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|█▎                                                             | 254/12579 [04:16<3:27:04,  0.99it/s, v_num=0, train_ce_loss=8.490]"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "checkpoint_callback = ModelCheckpoint(dirpath='./trained_models/', \n",
    "                                      filename=\"model_{epoch}-{step}_{train_ce_loss:.2f}\", \n",
    "                                      every_n_train_steps=200,\n",
    "                                      auto_insert_metric_name=True,\n",
    "                                      enable_version_counter=True)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=10, devices=1, accelerator='gpu', logger=True, log_every_n_steps=5, callbacks=[checkpoint_callback])\n",
=======
    "trainer = L.Trainer(max_epochs=10, devices=1, accelerator=\"gpu\")\n",
>>>>>>> dd25b937ffe06cfe8aecfaacad3380e27dfa4f7e
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2242e516-8933-415e-8b7d-41feb2d377b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403da1d-d6bd-4e58-bb27-f37d2460b1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
