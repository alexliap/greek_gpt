{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fcdcb92-3e24-455c-a5a2-7a55a618f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "from torch_implementation.language_model import LanguageModel\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, BatchSampler\n",
    "import lightning as L\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3c35bf4-d121-4fec-959e-9d01b9039ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_ENABLE_ONEDNN_OPTS=0\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "522a2468-9fc2-4120-956a-1f59792caee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = \"tokenizer_5000\"\n",
    "\n",
    "with open(f\"data/tokenized_data/train_{tokenizer}_data.pkl\", \"rb\") as file:\n",
    "    train_data = pickle.load(file)\n",
    "# with open(f\"data/tokenized_data/test_{tokenizer}_data.pkl\", \"rb\") as file:\n",
    "#     test_data = pickle.load(file)\n",
    "\n",
    "train_data = [token for sents in train_data for token in sents]\n",
    "# test_data = [token for sents in test_data for token in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b6620ca-845f-49db-8998-9d0d6ecda2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_data)\n",
    "# test_data = torch.tensor(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b814973-9831-414c-a77f-8b267b177c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dt = TensorDataset(train_data[:-1], train_data[1:])\n",
    "seq_sampler = SequentialSampler(train_dt)\n",
    "# rand_sampler = RandomSampler(train_dt, num_samples=3)\n",
    "batch_sampler = BatchSampler(seq_sampler, batch_size=128*256, drop_last=True)\n",
    "train_loader = DataLoader(train_dt, num_workers=10, batch_sampler=batch_sampler, persistent_workers=True)#, batch_size=128*256)\n",
    "\n",
    "model = LanguageModel(\n",
    "    vocab_size=5000,\n",
    "    n_embed=256,\n",
    "    context_len=256,\n",
    "    n_blocks=6,\n",
    "    n_heads=8,\n",
    "    n_experts=4,\n",
    "    top_k=2,\n",
    "    lr=3e-4,\n",
    ").bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9f27477-8118-4a3b-bd49-c74c6662d830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type              | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | embed_layer      | Embedding         | 1.3 M  | train\n",
      "1 | positional_embed | Embedding         | 65.5 K | train\n",
      "2 | blocks           | TransformerBlocks | 7.9 M  | train\n",
      "3 | layer_norm       | LayerNorm         | 512    | train\n",
      "4 | llm_head         | Linear            | 1.3 M  | train\n",
      "---------------------------------------------------------------\n",
      "10.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.5 M    Total params\n",
      "42.137    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9e32649d874cc2a3c788cb7a014f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _xla_gc_callback at 0x7ff10ce38b80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n",
      "    def _xla_gc_callback(*args):\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=10, devices=1, accelerator='gpu')\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1097792-ed01-4cea-9169-d9bd4a20af76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2242e516-8933-415e-8b7d-41feb2d377b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403da1d-d6bd-4e58-bb27-f37d2460b1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
